---
layout: page
title: Coursera Machine Learning Specialization Review part I
date: 2016-02-12
---
{{ page.date }}

Progress:
Two courses finished:

* Machine Learning Foundations: A Case Study Approach
* Machine Learning: Regression

Four more to go:

* Machine Learning: Classification
* Machine Learning: Clustering & Retrieval
* Machine Learning: Recommender Systems & Dimensionality Reduction
* Machine Learning Capstone: An Intelligent Application with Deep Learning 

University of Washington starting from January opened a machine learning specialization using Python.
It was right after the time when I finished the Machine Learning by Andrew Ng so I decided to jump straight in to
deepen my ML knowledge. It is projected that I can finish the specialization including the capstone project by June.

One sentence review: So far, I have finished the first two courses. Based on my experience, I think this specialization is worth spending more time
to actually finish.

`A Case Study Approach` gives students many different applications of ML techniques so that we know how we can apply
different algorithms at different times. This course is in general easy, as it only teaches about the use of different methods
such as classification and clustering. However one will be able to understand many different subfields along with their corresponding
applications after completing on course. The concepts mentioned will be studied in a more detail manner in the later courses, which will eventually
prepare the students with the knowledge to build a capstone project. I have to say, `A case study approach` is a good introductory course for this particular specialization.

`Regression` is the first more specific course in the specialization which deals with regression alone. Each week, there are lectures covering the
theoretical aspects and typically one quiz that tests the theoretical knowledge and one programming assignment that assess one's ability to
put the knowledge into action, using the algorithms via from GraphCreate or Sklearn, and implementing those algorithms from ground up.
I particularly enjoy this way of learning. Because first you use the in-place packages on some data set just to let you understand how the techniques work from a greater picture and then you are given the chance to implement the algorithms from the scratch to
really understand the math and concepts behind the scene, and therefore I do believe I have gained a greater understanding on different types of regressions after this course.

I very much look forward to the capstone project as it will encapsulate a wide range of topics and the students will have the freedom to the imagination and creativity to do something cool.

While taking MOOCs is one good way to get started with some subjects, I find it more helpful to actually apply those knowledge along with way. So
I entered Kaggle competition with some other fellows who are also taking MOOCs. Not long ago, we just used neural network and random forest on
the NIST data set. For the neural network, we used Google's TensorFlow which was quite fun to learn about. You can find the repo [here](https://github.com/TrueNorth-Kaggle/digitRecognizer) for detail. Indeed without taking those ML courses on Coursera, I wouldn't be able to understand how to use neural network on the digit data set at all. Soon, we will enter our first official competition, and I hope that will go well.

Cheers : )
